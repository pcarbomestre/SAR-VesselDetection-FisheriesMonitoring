{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcarbomestre/SAR-VesselDetection-FisheriesMonitoring/blob/main/prediction_to_coordinates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkGSYONQIln8"
      },
      "source": [
        "### Load libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTQQtWY02rtk"
      },
      "source": [
        "The virtual machines used in Colab for the runtimes are ephemeral, so some libraries not preinstalled in Colab environment have to be installed each time we open the notebook. More information about Colab [here](https://colab.research.google.com/github/tensorflow/workshops/blob/master/extras/amld/notebooks/solutions/0_colab.ipynb#scrollTo=fPP3Zw5iV2DP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqmzmV3i2uzR"
      },
      "source": [
        "Detectron2 is a library that provides state-of-the-art detection and segmentation algorithms. It is not installed by default in the Colab environmebt so we have to install it each time we open a session.\n",
        "\n",
        "More information about it at https://github.com/facebookresearch/detectron2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-r7DQmLeWf5m",
        "outputId": "723c6b5e-e87e-41b6-f23a-82bb63a8b010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-23hu863q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-23hu863q\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 94113be6e12db36b8c7601e13747587f19ec92fe\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.12.2)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black (from detectron2==0.6)\n",
            "  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.3)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (3.3.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.40.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=7420099 sha256=f7cfcd73bcaabe940910be77d7e52c446ab5c04c74a5cb706a30ab5059e5257e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yve7_30f/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=08d999874006e57aa80a14b1e0f268049a05eebfb0cce440ea8b91f222513cda\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=2d0f717440b49de038cdecf656df9d77edfd8109cefd9b55da76d9d21c074d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.3.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.1 portalocker-2.7.0 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install detectron2:\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "exit(0) # After installation restart Colab's runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrV8T3nPXnUm",
        "outputId": "13cbab97-ba8b-4965-fed6-9d9971434204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2022.12.7)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.3)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.22.4)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.3.7 snuggs-1.4.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyproj\n",
            "  Downloading pyproj-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyproj) (2022.12.7)\n",
            "Installing collected packages: pyproj\n",
            "Successfully installed pyproj-3.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyshp\n",
            "  Downloading pyshp-2.3.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyshp\n",
            "Successfully installed pyshp-2.3.1\n"
          ]
        }
      ],
      "source": [
        "# Install other required libraries\n",
        "!pip install rasterio\n",
        "!pip install pyproj\n",
        "!pip install pyshp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzsLh00-WS9x"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import torch, torchvision\n",
        "\n",
        "# Detectron2\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from cv2 import hconcat\n",
        "import matplotlib.pyplot as plt\n",
        "# Spatial data libraries\n",
        "import rasterio\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "from osgeo import gdal, osr\n",
        "import pandas as pd\n",
        "import tifffile\n",
        "import pyproj\n",
        "import shapefile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7K8hNYbW1tv"
      },
      "source": [
        "We also need to connect to our Google Drive accoung where we have stored our datasets, and where we are going to save our outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp3Iwn6iW1tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e83c7ca-3028-4bfb-e807-0a0cc43de588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (Import files from Google Drive in Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBD73ShzWS9y"
      },
      "source": [
        "# Set up and read in model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvCfbVX-YDdY"
      },
      "outputs": [],
      "source": [
        "# Model path\n",
        "model_path = '/content/drive/MyDrive/SSDD_pcarbomestre_2.0/output/ModelOutput/faster_rcnn_R_101_FPN_3x_26000iter/model_final.pth'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2611f8UWS9z",
        "outputId": "d2be8de5-9ace-4133-fbd5-4963585efa4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/06 20:35:55 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/drive/MyDrive/SSDD_pcarbomestre_2.0/output/ModelOutput/faster_rcnn_R_101_FPN_3x_26000iter/model_final.pth ...\n"
          ]
        }
      ],
      "source": [
        "# detectron2 configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))  # path to the YAML configuration file for a specific pre-trained model\n",
        "cfg.DATASETS.TRAIN = ()\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has class ship\n",
        "cfg.OUTPUT_DIR = \"output/\"\n",
        "cfg.MODEL.WEIGHTS = os.path.join(model_path) # Load custom weights\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # following Zhang et al. 2020\n",
        "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5  # following Zhang et al. 2020\n",
        "cfg.MODEL.DEVICE='cpu'\n",
        "\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvVBPC-bWS91"
      },
      "source": [
        "# Download images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Np9k9xWS92"
      },
      "source": [
        "Execute img_access.ipynb in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get image areas"
      ],
      "metadata": {
        "id": "lwfSfRfUOT-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_area(filepath):\n",
        "    # Open the GeoTIFF file\n",
        "    with rasterio.open(filepath) as ds:\n",
        "        # Get the georeferencing metadata\n",
        "        transform = ds.transform\n",
        "\n",
        "        # Calculate the area of a pixel\n",
        "        pixel_area = abs(transform[0] * transform[4] - transform[1] * transform[3])\n",
        "\n",
        "        # Read the raster data\n",
        "        data = ds.read(1)\n",
        "\n",
        "        # Get the number of non-NA pixels\n",
        "        total_pixels = np.count_nonzero(~np.isnan(data)) # & (data != 0)) # to exclude non-zero pixels\n",
        "\n",
        "        # Calculate the total area\n",
        "        total_area = total_pixels * pixel_area\n",
        "\n",
        "        # extract location from file name\n",
        "        location = os.path.basename(filepath).split('_')[0]\n",
        "        # extract date from file name\n",
        "        date_str = os.path.basename(filepath).split('_')[1]\n",
        "        # extract time from file name\n",
        "        time_str = os.path.basename(filepath).split('_')[2].split('.')[0]\n",
        "\n",
        "        # Create a dictionary to store the data\n",
        "        data = {\n",
        "            'location': location,\n",
        "            'date': date_str,\n",
        "            'time': time_str,\n",
        "            'area': total_area,\n",
        "            'pixel_count': total_pixels,\n",
        "            'pixel_area': pixel_area\n",
        "        }\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_directory(path):\n",
        "    # Create a list to store the data\n",
        "    data_list = []\n",
        "\n",
        "    # Traverse the directory structure\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        # Process all .tif files\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.tif'):\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                data = calculate_area(filepath)\n",
        "                data_list.append(data)\n",
        "\n",
        "    # Convert the list of dictionaries to a Pandas DataFrame\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    df.to_csv(\"/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/study_case/images_details/image_areas_20.csv\", index=False)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "gYtgy9VxOdgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_areas_df = process_directory(\"/content/drive/MyDrive/sentinel_images\")"
      ],
      "metadata": {
        "id": "GMSijXg_Ou4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get total images size"
      ],
      "metadata": {
        "id": "oyWa6tNEL26s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_files(folder_path):\n",
        "    count = 0\n",
        "    for _, _, files in os.walk(folder_path):\n",
        "        count += len(files)\n",
        "    return count\n",
        "\n",
        "folder_path = '/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/data/datasets/test'  # Replace with the actual folder path\n",
        "print(count_files(folder_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOzSnB7xs4zL",
        "outputId": "8a719f24-5d7d-43f2-a366-8f3ec1a60219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dir_size(path='.'):\n",
        "    total = 0\n",
        "    with os.scandir(path) as it:\n",
        "        for entry in it:\n",
        "            if entry.is_file():\n",
        "                total += entry.stat().st_size\n",
        "            elif entry.is_dir():\n",
        "                total += get_dir_size(entry.path)\n",
        "    return total\n",
        "\n",
        "print(get_dir_size('/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/data/datasets/test')/ 1073741824)\n"
      ],
      "metadata": {
        "id": "c0N71gCAL7RI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153b4a72-376b-4cba-f0a4-53fde7ff8bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2158882413059473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q39IpHfoWS93"
      },
      "source": [
        "# Filter images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GNbnponWS94"
      },
      "source": [
        "Filter images, to exclude those ones with incorrect shape. Subsequently, reproject all images to the same CRS of interes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCIqKQAdWS95"
      },
      "source": [
        "## Remove tif with invalid shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cSUTi2nWS96"
      },
      "outputs": [],
      "source": [
        "def get_tile_offsets(tif_path):\n",
        "    with tifffile.TiffFile(tif_path) as tif:\n",
        "        tags = tif.pages[0].tags\n",
        "        tile_offsets = tags[\"TileOffsets\"].value\n",
        "        return tile_offsets\n",
        "\n",
        "def remove_file_if_needed(tif_path):\n",
        "    tile_offsets = get_tile_offsets(tif_path)\n",
        "    if len(tile_offsets) <= 1:\n",
        "        os.remove(tif_path)\n",
        "        print(\"File removed:\", tif_path)\n",
        "    else:\n",
        "        print(\"File not removed:\", tif_path)\n",
        "\n",
        "def process_folder(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".tif\") or filename.endswith(\".tiff\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            remove_file_if_needed(file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U24jTZ1AWS97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d976100-e404-401f-84a6-a2a74c8fa90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-04_235745.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-11_234959.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-16_235745.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-23_234958.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-28_235744.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-02-04_234958.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-02-09_235744.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-02-16_234958.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-02-21_235744.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-02-28_234957.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-03-04_235744.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-03-11_234957.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-03-16_235744.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-03-23_234958.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-03-28_235744.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-04-04_234958.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-04-09_235744.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-04-16_234958.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-04-21_235745.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-04-28_234959.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-02_095013.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-07_095816.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-12_100624.tif\n",
            "File not removed: /content/drive/Shareddrives/ssdd_pcarbomestre/2020-1/corcovadogulf_2020-01-14_095012.tif\n"
          ]
        }
      ],
      "source": [
        "folder_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2020-1\"\n",
        "process_folder(folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtfpT9LdWS98"
      },
      "source": [
        "## Reproject"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_rasters_projection(images_folder, dst_crs='EPSG:4326'):\n",
        "    \"\"\"\n",
        "    Check if all raster files in a directory and its subdirectories are in a specific CRS.\n",
        "\n",
        "    Parameters:\n",
        "    images_folder: str\n",
        "        The directory to search for raster files.\n",
        "    dst_crs: str\n",
        "        The CRS to check against. Default is 'EPSG:4326' (WGS84).\n",
        "\n",
        "    Returns:\n",
        "    A list of raster files that are not in the specified CRS.\n",
        "    \"\"\"\n",
        "    # List to store the names of rasters not in the specified CRS\n",
        "    non_matching_rasters = []\n",
        "\n",
        "    # Walk through the directory\n",
        "    for root, dirs, files in os.walk(images_folder):\n",
        "        for file in files:\n",
        "            # If the file is a .tif file\n",
        "            if file.endswith('.tif'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Open the raster file\n",
        "                with rasterio.open(file_path) as src:\n",
        "                    # If the raster is not in the specified CRS\n",
        "                    if src.crs.to_string() != dst_crs:\n",
        "                        # Add the raster to the list\n",
        "                        non_matching_rasters.append(file)\n",
        "\n",
        "    # If all rasters are in the specified CRS\n",
        "    if not non_matching_rasters:\n",
        "        print(\"All raster files are in the desired projection: {}\".format(dst_crs))\n",
        "    # Return the list of non-matching rasters\n",
        "    return non_matching_rasters\n"
      ],
      "metadata": {
        "id": "uj6XL183jeMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_folder = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2020-1\"\n",
        "check_rasters_projection(images_folder, dst_crs='EPSG:4326')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSeU9gx1jhz1",
        "outputId": "cb047c7f-2c33-4813-e27d-a16b4b9c3ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corcovadogulf_2020-01-04_235745.tif',\n",
              " 'corcovadogulf_2020-01-11_234959.tif',\n",
              " 'corcovadogulf_2020-01-16_235745.tif',\n",
              " 'corcovadogulf_2020-01-23_234958.tif',\n",
              " 'corcovadogulf_2020-01-28_235744.tif',\n",
              " 'corcovadogulf_2020-02-04_234958.tif',\n",
              " 'corcovadogulf_2020-02-09_235744.tif',\n",
              " 'corcovadogulf_2020-02-16_234958.tif',\n",
              " 'corcovadogulf_2020-02-21_235744.tif',\n",
              " 'corcovadogulf_2020-02-28_234957.tif',\n",
              " 'corcovadogulf_2020-03-04_235744.tif',\n",
              " 'corcovadogulf_2020-03-11_234957.tif',\n",
              " 'corcovadogulf_2020-03-16_235744.tif',\n",
              " 'corcovadogulf_2020-03-23_234958.tif',\n",
              " 'corcovadogulf_2020-03-28_235744.tif',\n",
              " 'corcovadogulf_2020-04-04_234958.tif',\n",
              " 'corcovadogulf_2020-04-09_235744.tif',\n",
              " 'corcovadogulf_2020-04-16_234958.tif',\n",
              " 'corcovadogulf_2020-04-21_235745.tif',\n",
              " 'corcovadogulf_2020-04-28_234959.tif',\n",
              " 'corcovadogulf_2020-01-02_095013.tif',\n",
              " 'corcovadogulf_2020-01-07_095816.tif',\n",
              " 'corcovadogulf_2020-01-12_100624.tif',\n",
              " 'corcovadogulf_2020-01-14_095012.tif']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8DG6PDTWS98"
      },
      "source": [
        "Reproject all .tif to the same CRS of interest (e.g. EPSG:4326) so there is projection consistency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def reproject_all_to_wgs84(input_dir):\n",
        "    \"\"\"\n",
        "    Reproject all raster files in a directory to WGS84 (EPSG:4326) and overwrite them.\n",
        "\n",
        "    Parameters:\n",
        "    input_dir: str\n",
        "        Path to the directory containing raster files.\n",
        "    \"\"\"\n",
        "    dst_crs = 'EPSG:4326'  # WGS84\n",
        "\n",
        "    # Iterate over all files in the directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        # Process only files with .tif extension\n",
        "        if filename.endswith('.tif'):\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "\n",
        "            # Open the input file\n",
        "            with rasterio.open(input_path) as src:\n",
        "                transform, width, height = calculate_default_transform(\n",
        "                    src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
        "                kwargs = src.meta.copy()\n",
        "                kwargs.update({\n",
        "                    'crs': dst_crs,\n",
        "                    'transform': transform,\n",
        "                    'width': width,\n",
        "                    'height': height\n",
        "                })\n",
        "\n",
        "                # Create an in-memory raster file\n",
        "                dst_array = np.empty((src.count, height, width))\n",
        "                reproject(\n",
        "                    source=rasterio.band(src, range(1, src.count + 1)),\n",
        "                    destination=dst_array,\n",
        "                    src_transform=src.transform,\n",
        "                    src_crs=src.crs,\n",
        "                    dst_transform=transform,\n",
        "                    dst_crs=dst_crs,\n",
        "                    resampling=Resampling.nearest)\n",
        "\n",
        "            # Write the reprojected data to the input raster file, overwriting it\n",
        "            with rasterio.open(input_path, 'w', **kwargs) as dst:\n",
        "                dst.write(dst_array)\n"
      ],
      "metadata": {
        "id": "qD0d_YO4kkVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2020-1\"\n",
        "\n",
        "# Call the function\n",
        "reproject_all_to_wgs84(input_path)"
      ],
      "metadata": {
        "id": "Ht8P1m_AlNtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_rasters_projection(images_folder, dst_crs='EPSG:4326')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBQIe_6in0U1",
        "outputId": "3c001bd6-db60-484c-b354-b4417333f230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All raster files are in the desired projection: EPSG:4326\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqB99g-XWS99"
      },
      "source": [
        "# Detect vessels and extract coordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRR4uPpU6v9T"
      },
      "source": [
        "## Entire images analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aalxUOGaWS99"
      },
      "outputs": [],
      "source": [
        "def process_tiff_image(path, d2_predictor):\n",
        "    # Read the TIFF image with rasterio\n",
        "    with rasterio.open(path) as src:\n",
        "        img = src.read()\n",
        "        # Set the new minimum and maximum values\n",
        "        new_min = -20\n",
        "        new_max = 10\n",
        "        # Clip the pixel values to the new range\n",
        "        img_clipped = np.clip(img, new_min, new_max).astype(src.profile['dtype'])\n",
        "\n",
        "    # Transpose the image dimensions to BGR format (1, 4867, 4992) to (4867, 4992)\n",
        "    img_bgr = img_clipped.transpose(1, 2, 0)[:, :, ::-1]\n",
        "\n",
        "    # Rescale the image to 0-255 range and transform dtype from float64 to uint8\n",
        "    img_bgr = cv2.normalize(img_bgr, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "    # Convert the image from grayscale to BGR format (4867, 4992, 3)\n",
        "    img_bgr_new = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Make predictions on the image\n",
        "    outputs = d2_predictor(img_bgr_new)\n",
        "\n",
        "    # Get the bounding boxes for the predicted objects\n",
        "    boxes = outputs['instances'].pred_boxes.tensor.cpu().numpy()\n",
        "    scores = outputs['instances'].scores.cpu().numpy()\n",
        "\n",
        "    # Define the georeferencing transformation function\n",
        "    def pixel_to_geo(x, y):\n",
        "        geo_x = x * gt[1] + gt[0]\n",
        "        geo_y = y * gt[5] + gt[3]\n",
        "        return geo_x, geo_y\n",
        "\n",
        "    # Load the georeferencing information from the image\n",
        "    ds = gdal.Open(path)\n",
        "    gt = ds.GetGeoTransform()\n",
        "    proj = ds.GetProjection()\n",
        "\n",
        "    # Define the object containing the bounding boxes\n",
        "    bbox_array = boxes\n",
        "\n",
        "    # Calculate the centroid of each bounding box in geographic coordinates\n",
        "    centroid_array = np.zeros((len(bbox_array), 2))\n",
        "    for i, bbox in enumerate(bbox_array):\n",
        "        xmin, ymin, xmax, ymax = bbox\n",
        "        geo_xmin, geo_ymin = pixel_to_geo(xmin, ymin)\n",
        "        geo_xmax, geo_ymax = pixel_to_geo(xmax, ymax)\n",
        "        centroid_array[i, 0] = (geo_xmin + geo_xmax) / 2\n",
        "        centroid_array[i, 1] = (geo_ymin + geo_ymax) / 2\n",
        "\n",
        "    # Create a list to store the detected objects\n",
        "    detected_objects = []\n",
        "\n",
        "    filename = ds.GetDescription()\n",
        "    # extract location from file name\n",
        "    location = filename.split('/')[-1].split('_')[0]\n",
        "    # extract date from file name\n",
        "    date_str = filename.split('/')[-1].split('_')[1]\n",
        "    # extract time from file name\n",
        "    time_str = filename.split('/')[-1].split('_')[2].split('.')[0]\n",
        "\n",
        "    for i, centroid in enumerate(centroid_array):\n",
        "        x, y = centroid\n",
        "        # get the score values for the current point\n",
        "        current_score = scores[i]\n",
        "        # Create a dictionary to store the object's attributes\n",
        "        obj_dict = {\n",
        "            'centroid': (x, y),\n",
        "            'location': location,\n",
        "            'date': date_str,\n",
        "            'time': time_str,\n",
        "            'score': current_score\n",
        "        }\n",
        "        # Append the dictionary to detected_objects\n",
        "        detected_objects.append(obj_dict)\n",
        "\n",
        "    return(detected_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1BtsA0DWS9-"
      },
      "outputs": [],
      "source": [
        "def process_folder(path, d2_predictor):\n",
        "    # Create a list to store all detected objects\n",
        "    detected_objects_all = []\n",
        "    # Iterate over all files in the folder\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".tif\"):\n",
        "            filepath = os.path.join(path, filename)\n",
        "            # Process the TIFF image\n",
        "            detected_objects = process_tiff_image(filepath, d2_predictor)\n",
        "            # Add the detected objects to the list\n",
        "            detected_objects_all.extend(detected_objects)\n",
        "\n",
        "    # Save the centroid coordinates as points in a shapefile\n",
        "    sf_path = '/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/study_case/shapefiles/'\n",
        "    sf = shapefile.Writer(sf_path + 'centroids')\n",
        "    sf.field('location', 'C', size=50) # add a new field for location\n",
        "    sf.field('date', 'C', size=50) # add a new field for date\n",
        "    sf.field('time', 'C', size=50) # add a new field for time\n",
        "    sf.field('score', 'F', size=10, decimal=9) # add a new field for score\n",
        "    sf.autoBalance = 1\n",
        "\n",
        "    # Add points to the Shapefile\n",
        "    for obj in detected_objects_all:\n",
        "        centroid = obj['centroid']\n",
        "        location = obj['location']\n",
        "        date = obj['date']\n",
        "        time = obj['time']\n",
        "        score = obj['score']\n",
        "\n",
        "        # Add the point to the Shapefile\n",
        "        sf.point(*centroid)\n",
        "        # Add the attributes to the Shapefile\n",
        "        sf.record(location, date, time, score)\n",
        "    sf.close()\n",
        "\n",
        "    # Set the projection of the shapefile\n",
        "    prj_content = pyproj.CRS.from_string('EPSG:4326').to_wkt()\n",
        "    with open(sf_path + 'centroids' + '.prj', 'w') as prj_file:\n",
        "        prj_file.write(prj_content)\n",
        "    # Save and close the shapefile\n",
        "    sf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9zkXogeWS9_"
      },
      "outputs": [],
      "source": [
        "process_folder('/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/study_case/sentinel_images', predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOHgWIII614t"
      },
      "source": [
        "## Subsetting analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN7k2hAJ68tV"
      },
      "outputs": [],
      "source": [
        "def process_tiff_image_subsetting(path, d2_predictor):\n",
        "    # Read the TIFF image with rasterio\n",
        "    with rasterio.open(path) as src:\n",
        "        img = src.read()\n",
        "        # Set the new minimum and maximum values\n",
        "        new_min = -20\n",
        "        new_max = 10\n",
        "        # Clip the pixel values to the new range\n",
        "        img_clipped = np.clip(img, new_min, new_max).astype(src.profile['dtype'])\n",
        "\n",
        "    # Transpose the image dimensions to BGR format (1, 4867, 4992) to (4867, 4992)\n",
        "    img_bgr = img_clipped.transpose(1, 2, 0)[:, :, ::-1]\n",
        "\n",
        "    # Rescale the image to 0-255 range and transform dtype from float64 to uint8\n",
        "    img_bgr = cv2.normalize(img_bgr, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "    # Convert the image from grayscale to BGR format (4867, 4992, 3)\n",
        "    img_bgr_new = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Define the size of the sub-images\n",
        "    sub_img_height = 800\n",
        "    sub_img_width = 800\n",
        "\n",
        "    # Get the dimensions of the original image\n",
        "    img_height, img_width, _ = img_bgr_new.shape\n",
        "\n",
        "    # Compute the number of sub-images needed to cover the original image\n",
        "    num_sub_images_height = int(np.ceil(img_height / sub_img_height))\n",
        "    num_sub_images_width = int(np.ceil(img_width / sub_img_width))\n",
        "\n",
        "    # Create a list to hold the predictions for each sub-image\n",
        "    all_boxes = []\n",
        "    all_scores = []\n",
        "\n",
        "    # Loop over the sub-images\n",
        "    for i in range(num_sub_images_height):\n",
        "        for j in range(num_sub_images_width):\n",
        "            # Compute the starting and ending indices for the sub-image\n",
        "            start_h = i * sub_img_height\n",
        "            end_h = min((i + 1) * sub_img_height, img_height)\n",
        "            start_w = j * sub_img_width\n",
        "            end_w = min((j + 1) * sub_img_width, img_width)\n",
        "\n",
        "            # Extract the sub-image\n",
        "            sub_img = img_bgr_new[start_h:end_h, start_w:end_w, :]\n",
        "\n",
        "            # Make predictions on the sub-image\n",
        "            outputs = d2_predictor(sub_img)\n",
        "\n",
        "            # Get the bounding boxes and scores for the predicted objects\n",
        "            boxes = outputs['instances'].pred_boxes.tensor.cpu().numpy()\n",
        "            scores = outputs['instances'].scores.cpu().numpy()\n",
        "\n",
        "            # Convert the bounding box coordinates from sub-image to original image coordinates\n",
        "            boxes[:, 0::2] += start_w\n",
        "            boxes[:, 1::2] += start_h\n",
        "\n",
        "            # Add the bounding boxes and scores to the list for all sub-images\n",
        "            all_boxes.append(boxes)\n",
        "            all_scores.append(scores)\n",
        "\n",
        "    # Concatenate the bounding boxes and scores for all sub-images\n",
        "    all_boxes = np.concatenate(all_boxes, axis=0)\n",
        "    all_scores = np.concatenate(all_scores, axis=0)\n",
        "\n",
        "    # Define the georeferencing transformation function\n",
        "    def pixel_to_geo(x, y):\n",
        "        geo_x = x * gt[1] + gt[0]\n",
        "        geo_y = y * gt[5] + gt[3]\n",
        "        return geo_x, geo_y\n",
        "\n",
        "    # Load the georeferencing information from the image\n",
        "    ds = gdal.Open(path)\n",
        "    gt = ds.GetGeoTransform()\n",
        "    proj = ds.GetProjection()\n",
        "\n",
        "\n",
        "    # Define the object containing the bounding boxes\n",
        "    bbox_array = all_boxes\n",
        "\n",
        "    # Calculate the centroid of each bounding box in geographic coordinates\n",
        "    centroid_array = np.zeros((len(bbox_array), 2))\n",
        "    for i, bbox in enumerate(bbox_array):\n",
        "        xmin, ymin, xmax, ymax = bbox\n",
        "        geo_xmin, geo_ymin = pixel_to_geo(xmin, ymin)\n",
        "        geo_xmax, geo_ymax = pixel_to_geo(xmax, ymax)\n",
        "        centroid_array[i, 0] = (geo_xmin + geo_xmax) / 2\n",
        "        centroid_array[i, 1] = (geo_ymin + geo_ymax) / 2\n",
        "\n",
        "    # Create a list to store the detected objects\n",
        "    detected_objects = []\n",
        "\n",
        "    filename = ds.GetDescription()\n",
        "    # extract location from file name\n",
        "    location = filename.split('/')[-1].split('_')[0]\n",
        "    # extract date from file name\n",
        "    date_str = filename.split('/')[-1].split('_')[1]\n",
        "    # extract time from file name\n",
        "    time_str = filename.split('/')[-1].split('_')[2].split('.')[0]\n",
        "\n",
        "    for i, centroid in enumerate(centroid_array):\n",
        "        x, y = centroid\n",
        "        # get the score values for the current point\n",
        "        current_score = all_scores[i]\n",
        "        # Create a dictionary to store the object's attributes\n",
        "        obj_dict = {\n",
        "            'centroid': (x, y),\n",
        "            'location': location,\n",
        "            'date': date_str,\n",
        "            'time': time_str,\n",
        "            'score': current_score\n",
        "        }\n",
        "        # Append the dictionary to detected_objects\n",
        "        detected_objects.append(obj_dict)\n",
        "\n",
        "    return(detected_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1xrkycq68tX"
      },
      "outputs": [],
      "source": [
        "def process_folder_subsetting(path, d2_predictor):\n",
        "    # Create a list to store all detected objects\n",
        "    detected_objects_all = []\n",
        "    # Iterate over all files in the folder\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".tif\"):\n",
        "            filepath = os.path.join(path, filename)\n",
        "            # Process the TIFF image\n",
        "            print(filename)\n",
        "            detected_objects = process_tiff_image_subsetting(filepath, d2_predictor)\n",
        "            # Add the detected objects to the list\n",
        "            detected_objects_all.extend(detected_objects)\n",
        "\n",
        "    # Save the centroid coordinates as points in a shapefile\n",
        "    sf_path = '/content/drive/MyDrive/SSDD_pcarbomestre_2.0/study_case/shapefiles/'\n",
        "    sf = shapefile.Writer(sf_path + 'centroids_sub')\n",
        "    sf.field('location', 'C', size=50) # add a new field for location\n",
        "    sf.field('date', 'C', size=50) # add a new field for date\n",
        "    sf.field('time', 'C', size=50) # add a new field for time\n",
        "    sf.field('score', 'F', size=10, decimal=9) # add a new field for score\n",
        "    sf.autoBalance = 1\n",
        "\n",
        "    # Add points to the Shapefile\n",
        "    for obj in detected_objects_all:\n",
        "        centroid = obj['centroid']\n",
        "        location = obj['location']\n",
        "        date = obj['date']\n",
        "        time = obj['time']\n",
        "        score = obj['score']\n",
        "\n",
        "        # Add the point to the Shapefile\n",
        "        sf.point(*centroid)\n",
        "        # Add the attributes to the Shapefile\n",
        "        sf.record(location, date, time, score)\n",
        "    sf.close()\n",
        "\n",
        "    # Set the projection of the shapefile\n",
        "    prj_content = pyproj.CRS.from_string('EPSG:4326').to_wkt()\n",
        "    with open(sf_path + 'centroids_sub' + '.prj', 'w') as prj_file:\n",
        "        prj_file.write(prj_content)\n",
        "    # Save and close the shapefile\n",
        "    sf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQM-wIrD68tY",
        "outputId": "2fcf13b3-fb08-4199-f62e-bb85237ca290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corcovadogulf_2020-01-04_235745.tif\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corcovadogulf_2020-01-11_234959.tif\n",
            "corcovadogulf_2020-01-16_235745.tif\n",
            "corcovadogulf_2020-01-23_234958.tif\n",
            "corcovadogulf_2020-01-28_235744.tif\n",
            "corcovadogulf_2020-02-04_234958.tif\n",
            "corcovadogulf_2020-02-09_235744.tif\n",
            "corcovadogulf_2020-02-16_234958.tif\n",
            "corcovadogulf_2020-02-21_235744.tif\n",
            "corcovadogulf_2020-02-28_234957.tif\n",
            "corcovadogulf_2020-03-04_235744.tif\n",
            "corcovadogulf_2020-03-11_234957.tif\n",
            "corcovadogulf_2020-03-16_235744.tif\n",
            "corcovadogulf_2020-03-23_234958.tif\n",
            "corcovadogulf_2020-03-28_235744.tif\n",
            "corcovadogulf_2020-04-04_234958.tif\n",
            "corcovadogulf_2020-04-09_235744.tif\n",
            "corcovadogulf_2020-04-16_234958.tif\n",
            "corcovadogulf_2020-04-21_235745.tif\n",
            "corcovadogulf_2020-04-28_234959.tif\n",
            "corcovadogulf_2020-01-02_095013.tif\n",
            "corcovadogulf_2020-01-07_095816.tif\n",
            "corcovadogulf_2020-01-12_100624.tif\n",
            "corcovadogulf_2020-01-14_095012.tif\n"
          ]
        }
      ],
      "source": [
        "input_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2020-1\"\n",
        "\n",
        "process_folder_subsetting(input_path, predictor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0N8eN1WnIjwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kf1hFJOqOcd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tiff_image_subsetting(path, d2_predictor):\n",
        "   # Reproject the image to EPSG:4326\n",
        "    src = reproject_tiff(path)\n",
        "\n",
        "    img = src.ReadAsArray()\n",
        "\n",
        "    # Read the TIFF image with rasterio\n",
        "    with rasterio.open(path) as src:\n",
        "        img = src.read()\n",
        "        # Set the new minimum and maximum values\n",
        "        new_min = -20\n",
        "        new_max = 10\n",
        "        # Clip the pixel values to the new range\n",
        "        img_clipped = np.clip(img, new_min, new_max).astype(src.profile['dtype'])\n",
        "\n",
        "    # Transpose the image dimensions to BGR format (1, 4867, 4992) to (4867, 4992)\n",
        "    img_bgr = img_clipped.transpose(1, 2, 0)[:, :, ::-1]\n",
        "\n",
        "    # Rescale the image to 0-255 range and transform dtype from float64 to uint8\n",
        "    img_bgr = cv2.normalize(img_bgr, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "    # Convert the image from grayscale to BGR format (4867, 4992, 3)\n",
        "    img_bgr_new = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Define the size of the sub-images\n",
        "    sub_img_height = 800\n",
        "    sub_img_width = 800\n",
        "\n",
        "    # Get the dimensions of the original image\n",
        "    img_height, img_width, _ = img_bgr_new.shape\n",
        "\n",
        "    # Compute the number of sub-images needed to cover the original image\n",
        "    num_sub_images_height = int(np.ceil(img_height / sub_img_height))\n",
        "    num_sub_images_width = int(np.ceil(img_width / sub_img_width))\n",
        "\n",
        "    # Create a list to hold the predictions for each sub-image\n",
        "    all_boxes = []\n",
        "    all_scores = []\n",
        "\n",
        "    # Loop over the sub-images\n",
        "    for i in range(num_sub_images_height):\n",
        "        for j in range(num_sub_images_width):\n",
        "            # Compute the starting and ending indices for the sub-image\n",
        "            start_h = i * sub_img_height\n",
        "            end_h = min((i + 1) * sub_img_height, img_height)\n",
        "            start_w = j * sub_img_width\n",
        "            end_w = min((j + 1) * sub_img_width, img_width)\n",
        "\n",
        "            # Extract the sub-image\n",
        "            sub_img = img_bgr_new[start_h:end_h, start_w:end_w, :]\n",
        "\n",
        "            # Make predictions on the sub-image\n",
        "            outputs = d2_predictor(sub_img)\n",
        "\n",
        "            # Get the bounding boxes and scores for the predicted objects\n",
        "            boxes = outputs['instances'].pred_boxes.tensor.cpu().numpy()\n",
        "            scores = outputs['instances'].scores.cpu().numpy()\n",
        "\n",
        "            # Convert the bounding box coordinates from sub-image to original image coordinates\n",
        "            boxes[:, 0::2] += start_w\n",
        "            boxes[:, 1::2] += start_h\n",
        "\n",
        "            # Add the bounding boxes and scores to the list for all sub-images\n",
        "            all_boxes.append(boxes)\n",
        "            all_scores.append(scores)\n",
        "\n",
        "    # Concatenate the bounding boxes and scores for all sub-images\n",
        "    all_boxes = np.concatenate(all_boxes, axis=0)\n",
        "    all_scores = np.concatenate(all_scores, axis=0)\n",
        "\n",
        "    # Define the georeferencing transformation function\n",
        "    def pixel_to_geo(x, y):\n",
        "        geo_x = x * gt[1] + gt[0]\n",
        "        geo_y = y * gt[5] + gt[3]\n",
        "        return geo_x, geo_y\n",
        "\n",
        "    # Load the georeferencing information from the image\n",
        "    ds = gdal.Open(path)\n",
        "    gt = ds.GetGeoTransform()\n",
        "    proj = ds.GetProjection()\n",
        "\n",
        "\n",
        "    # Define the object containing the bounding boxes\n",
        "    bbox_array = all_boxes\n",
        "\n",
        "    # Calculate the centroid of each bounding box in geographic coordinates\n",
        "    centroid_array = np.zeros((len(bbox_array), 2))\n",
        "    for i, bbox in enumerate(bbox_array):\n",
        "        xmin, ymin, xmax, ymax = bbox\n",
        "        geo_xmin, geo_ymin = pixel_to_geo(xmin, ymin)\n",
        "        geo_xmax, geo_ymax = pixel_to_geo(xmax, ymax)\n",
        "        centroid_array[i, 0] = (geo_xmin + geo_xmax) / 2\n",
        "        centroid_array[i, 1] = (geo_ymin + geo_ymax) / 2\n",
        "\n",
        "    # Create a list to store the detected objects\n",
        "    detected_objects = []\n",
        "\n",
        "    filename = ds.GetDescription()\n",
        "    # extract location from file name\n",
        "    location = filename.split('/')[-1].split('_')[0]\n",
        "    # extract date from file name\n",
        "    date_str = filename.split('/')[-1].split('_')[1]\n",
        "    # extract time from file name\n",
        "    time_str = filename.split('/')[-1].split('_')[2].split('.')[0]\n",
        "\n",
        "    for i, centroid in enumerate(centroid_array):\n",
        "        x, y = centroid\n",
        "        # get the score values for the current point\n",
        "        current_score = all_scores[i]\n",
        "        # Create a dictionary to store the object's attributes\n",
        "        obj_dict = {\n",
        "            'centroid': (x, y),\n",
        "            'location': location,\n",
        "            'date': date_str,\n",
        "            'time': time_str,\n",
        "            'score': current_score\n",
        "        }\n",
        "        # Append the dictionary to detected_objects\n",
        "        detected_objects.append(obj_dict)\n",
        "\n",
        "    return(detected_objects)"
      ],
      "metadata": {
        "id": "mvXedQbwUiKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def process_folder_subsetting(path, d2_predictor):\n",
        "    # Create a list to store all detected objects\n",
        "    detected_objects_all = []\n",
        "\n",
        "    # Iterate over all files in the folder\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".tif\"):\n",
        "            filepath = os.path.join(path, filename)\n",
        "            # Process the TIFF image\n",
        "            print(filename)\n",
        "            detected_objects = process_tiff_image_subsetting(filepath, d2_predictor)\n",
        "            # Add the detected objects to the list\n",
        "            detected_objects_all.extend(detected_objects)\n",
        "\n",
        "    # Save the data in a CSV file\n",
        "    csv_path = '/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/study_case/csv/'\n",
        "    with open(csv_path + 'centroids_sub.csv', 'w', newline='') as csvfile:\n",
        "        fieldnames = ['centroid_x', 'centroid_y', 'location', 'date', 'time', 'score']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for obj in detected_objects_all:\n",
        "            centroid = obj['centroid']\n",
        "            location = obj['location']\n",
        "            date = obj['date']\n",
        "            time = obj['time']\n",
        "            score = obj['score']\n",
        "\n",
        "            # Write the attributes to the CSV file\n",
        "            writer.writerow({'centroid_x': centroid[0], 'centroid_y': centroid[1], 'location': location, 'date': date, 'time': time, 'score': score})\n"
      ],
      "metadata": {
        "id": "bQyhEsM9WkJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reproject_tiff(path):\n",
        "    # Define target SRS\n",
        "    dst_srs = osr.SpatialReference()\n",
        "    dst_srs.ImportFromEPSG(4326)  # WGS 84\n",
        "\n",
        "    # Load source image\n",
        "    src_ds = gdal.Open(path)\n",
        "    src_srs = osr.SpatialReference()\n",
        "    src_srs.ImportFromWkt(src_ds.GetProjection())\n",
        "\n",
        "    # Reproject in memory\n",
        "    mem_ds = gdal.Warp('', src_ds, dstSRS=dst_srs, format='MEM')\n",
        "\n",
        "    return mem_ds\n"
      ],
      "metadata": {
        "id": "N7uV_C8Ea87Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_folder_subsetting('/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/study_case/sentinel_images/2018-1/test', predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v7Vya4PQ-8S",
        "outputId": "121a6aa6-ede0-40cd-e33b-3e7494ad523e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corcovadogulf_2018-01-03_234904.tif\n",
            "corcovadogulf_2018-02-17_094941.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_tiff_image_subsetting('/content/drive/Shareddrives/ssdd_pcarbomestre/SSDD_pcarbomestre_2.0/study_case/sentinel_images/2018-1/test/Copyofcorcovadogulf_2018-02-17_094941.tif', predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XayWDEhFYbwk",
        "outputId": "54513f84-4b2a-4855-ac88-c0c1e3f09cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'centroid': (-73.01604260633118, -43.60991769047326),\n",
              "  'location': 'Copyofcorcovadogulf',\n",
              "  'date': '2018-02-17',\n",
              "  'time': '094941',\n",
              "  'score': 0.6000059}]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reproject_to_wgs84(input_path):\n",
        "    \"\"\"\n",
        "    Reproject a raster file to WGS84 (EPSG:4326) and overwrite the input.\n",
        "\n",
        "    Parameters:\n",
        "    input_path: str\n",
        "        Path to the input raster file.\n",
        "    \"\"\"\n",
        "    dst_crs = 'EPSG:4326'  # WGS84\n",
        "\n",
        "    # Open the input file\n",
        "    with rasterio.open(input_path) as src:\n",
        "        transform, width, height = calculate_default_transform(\n",
        "            src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
        "        kwargs = src.meta.copy()\n",
        "        kwargs.update({\n",
        "            'crs': dst_crs,\n",
        "            'transform': transform,\n",
        "            'width': width,\n",
        "            'height': height\n",
        "        })\n",
        "\n",
        "        # Create an in-memory raster file\n",
        "        dst_array = np.empty((src.count, height, width))\n",
        "        reproject(\n",
        "            source=rasterio.band(src, range(1, src.count + 1)),\n",
        "            destination=dst_array,\n",
        "            src_transform=src.transform,\n",
        "            src_crs=src.crs,\n",
        "            dst_transform=transform,\n",
        "            dst_crs=dst_crs,\n",
        "            resampling=Resampling.nearest)\n",
        "\n",
        "    # Write the reprojected data to the input raster file, overwriting it\n",
        "    with rasterio.open(input_path, 'w', **kwargs) as dst:\n",
        "        dst.write(dst_array)\n"
      ],
      "metadata": {
        "id": "T5aoS37dbEki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2018-1/corcovadogulf_2018-01-05_095748.tif\"\n",
        "output_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2018-1/corcovadogulf_2018-01-05_095748_repro.tif\"\n",
        "\n",
        "# Call the function\n",
        "reproject_to_wgs84(input_path)"
      ],
      "metadata": {
        "id": "qJwOB2Vt6Kki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import rasterio\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "\n",
        "def reproject_tif(input_tif_path, output_folder):\n",
        "    # Construct the output file path\n",
        "    output_tif_path = os.path.join(output_folder)\n",
        "\n",
        "    # Open the input file\n",
        "    with rasterio.open(input_tif_path) as src:\n",
        "        # Read the original CRS and data\n",
        "        crs = src.crs\n",
        "\n",
        "        # Calculate the transform and width and height for the new CRS\n",
        "        transform, width, height = calculate_default_transform(crs, 'EPSG:4326', src.width, src.height, *src.bounds)\n",
        "\n",
        "        # Define the metadata for the output file\n",
        "        kwargs = src.meta.copy()\n",
        "        kwargs.update({\n",
        "            'crs': 'EPSG:4326',\n",
        "            'transform': transform,\n",
        "            'width': width,\n",
        "            'height': height\n",
        "        })\n",
        "\n",
        "        # Open the output file and reproject the input data into it\n",
        "        with rasterio.open(output_tif_path, 'w', **kwargs) as dst:\n",
        "            for i in range(1, src.count + 1):\n",
        "                reproject(\n",
        "                    source=rasterio.band(src, i),\n",
        "                    destination=rasterio.band(dst, i),\n",
        "                    src_transform=src.transform,\n",
        "                    src_crs=src.crs,\n",
        "                    dst_transform=transform,\n",
        "                    dst_crs='EPSG:4326',\n",
        "                    resampling=Resampling.nearest)\n",
        "\n"
      ],
      "metadata": {
        "id": "LSBTKvIdyrl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2018-1/corcovadogulf_2018-01-05_095748.tif\"\n",
        "output_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2018-1/corcovadogulf_2018-01-05_095748_repro.tif\"\n",
        "\n",
        "# Call the function\n",
        "reproject_tif(input_path, output_path)"
      ],
      "metadata": {
        "id": "6UTFTSGqgsqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a36MGxRJh2t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def reproject_all_to_wgs84(input_dir):\n",
        "    \"\"\"\n",
        "    Reproject all raster files in a directory to WGS84 (EPSG:4326) and overwrite them.\n",
        "\n",
        "    Parameters:\n",
        "    input_dir: str\n",
        "        Path to the directory containing raster files.\n",
        "    \"\"\"\n",
        "    dst_crs = 'EPSG:4326'  # WGS84\n",
        "\n",
        "    # Iterate over all files in the directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        # Process only files with .tif extension\n",
        "        if filename.endswith('.tif'):\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "\n",
        "            # Open the input file\n",
        "            with rasterio.open(input_path) as src:\n",
        "                transform, width, height = calculate_default_transform(\n",
        "                    src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
        "                kwargs = src.meta.copy()\n",
        "                kwargs.update({\n",
        "                    'crs': dst_crs,\n",
        "                    'transform': transform,\n",
        "                    'width': width,\n",
        "                    'height': height\n",
        "                })\n",
        "\n",
        "                # Create an in-memory raster file\n",
        "                dst_array = np.empty((src.count, height, width))\n",
        "                reproject(\n",
        "                    source=rasterio.band(src, range(1, src.count + 1)),\n",
        "                    destination=dst_array,\n",
        "                    src_transform=src.transform,\n",
        "                    src_crs=src.crs,\n",
        "                    dst_transform=transform,\n",
        "                    dst_crs=dst_crs,\n",
        "                    resampling=Resampling.nearest)\n",
        "\n",
        "            # Write the reprojected data to the input raster file, overwriting it\n",
        "            with rasterio.open(input_path, 'w', **kwargs) as dst:\n",
        "                dst.write(dst_array)\n"
      ],
      "metadata": {
        "id": "ZXARMvUG7N8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/Shareddrives/ssdd_pcarbomestre/2018-1\"\n",
        "\n",
        "# Call the function\n",
        "reproject_all_to_wgs84(input_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "boQlCS0G7RM0",
        "outputId": "5d572838-9f33-4f70-e888-970d4b2c5cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d6b1e37d79c3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Call the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mreproject_all_to_wgs84\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-30e0e1e40f63>\u001b[0m in \u001b[0;36mreproject_all_to_wgs84\u001b[0;34m(input_dir)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Write the reprojected data to the input raster file, overwriting it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__exit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.close\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreceived_exc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msuppressed_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;34m\"\"\"Immediately unwind the context stack.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HCIqKQAdWS95",
        "gRR4uPpU6v9T"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "uoc_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}